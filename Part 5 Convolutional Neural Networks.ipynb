{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Things to import\n",
    "\n",
    "# Standard data, plotting, and mathematical tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data for BTC\n",
    "\n",
    "dfs=['PCA 24 Hourly BTC.csv', 'PCA 24 Hourly ETH.csv', 'PCA 24 Hourly ADA.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating \"images\", training, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (30560, 20, 20, 1)\n",
      "30560 train samples\n",
      "7641 test samples\n",
      "[[[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]\n",
      "\n",
      " [[-3.33534097]\n",
      "  [ 5.5909635 ]\n",
      "  [-0.14421798]\n",
      "  [-1.16194967]\n",
      "  [ 1.64008498]\n",
      "  [ 0.21644828]\n",
      "  [ 3.26762363]\n",
      "  [ 1.09498097]\n",
      "  [ 0.28304689]\n",
      "  [-0.11291285]\n",
      "  [-0.13932107]\n",
      "  [ 1.51396396]\n",
      "  [-1.50044942]\n",
      "  [-1.47739749]\n",
      "  [ 0.23746479]\n",
      "  [ 0.42721741]\n",
      "  [ 1.0997628 ]\n",
      "  [-0.97500528]\n",
      "  [ 1.38130737]\n",
      "  [ 0.23089355]]]\n"
     ]
    }
   ],
   "source": [
    "# Getting our X and y prepared to make images\n",
    "\n",
    "X=pd.read_csv(dfs[0])\n",
    "y=X['Label']\n",
    "X=X.drop('Label', axis=1)\n",
    "X=X.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Creating 20x20 stacks of each row to act as images\n",
    "\n",
    "X_images_v=[]\n",
    "for i in range(len(X)):\n",
    "    X_img_init=X.iloc[i].values\n",
    "    X_img=X.iloc[i].values\n",
    "    for i in range(19):\n",
    "        X_img=np.vstack((X_img,X_img_init))\n",
    "    X_images_v.append(X_img)\n",
    "    \n",
    "# Training and test data\n",
    "    \n",
    "X_train, X_test, y_train, y_test=train_test_split(X_images_v, y, test_size=0.2, random_state=100, shuffle=True)\n",
    "\n",
    "# Finalizing the shapes\n",
    "\n",
    "num_classes = 3\n",
    "input_shape = (20, 20, 1)\n",
    "\n",
    "# Add a dimension at the end to make sure images have shape (28, 28, 1)\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "print(\"x_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_test.shape[0], \"test samples\")\n",
    "# one-hot-encode the target variable\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split into train and test\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_images, y, test_size=0.2, random_state=100, shuffle=True)\n",
    "# X_train, X_test, y_train, y_test=train_test_split(X_train, y_train, test_size=0.2, random_state=100, shuffle=True)\n",
    "# print(len(X_train)) # Right size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 3\n",
    "# input_shape = (20, 20, 1)\n",
    "\n",
    "# # Add a dimension at the end to make sure images have shape (28, 28, 1)\n",
    "# X_train = np.expand_dims(X_train, -1)\n",
    "# X_test = np.expand_dims(X_test, -1)\n",
    "# print(\"x_train shape:\", X_train.shape)\n",
    "# print(X_train.shape[0], \"train samples\")\n",
    "# print(X_test.shape[0], \"test samples\")\n",
    "# # one-hot-encode the target variable\n",
    "# y_train = utils.to_categorical(y_train, num_classes)\n",
    "# y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Grid Search with an example pulled from the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to create the model for Keras wrapper to scikit learn\n",
    "# # we will optimize the type of pooling layer (max or average) and the activation function of the 2nd and 3rd convolution layers \n",
    "# def create_cnn_model(pool_type='max', conv_activation='sigmoid', dropout_rate=0.10):\n",
    "#     # create model\n",
    "#     model = keras.Sequential()\n",
    "    \n",
    "#     # first layer: convolution\n",
    "#     model.add(keras.layers.Conv2D(16, kernel_size=(5, 5), activation='relu', input_shape=input_shape)) \n",
    "        \n",
    "#     # second series of layers: convolution, pooling, and dropout\n",
    "#     model.add(keras.layers.Conv2D(32, kernel_size=(5, 5), activation=conv_activation))  \n",
    "#     if pool_type == 'max':\n",
    "#         model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#     if pool_type == 'average':\n",
    "#         model.add(keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "#     if dropout_rate != 0:\n",
    "#         model.add(keras.layers.Dropout(rate=dropout_rate))     \n",
    "    \n",
    "#     # third series of layers: convolution, pooling, and dropout    \n",
    "#     model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation=conv_activation))   # 32   \n",
    "#     if pool_type == 'max':\n",
    "#         model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#     if pool_type == 'average':\n",
    "#         model.add(keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "#     if dropout_rate != 0:\n",
    "#         model.add(keras.layers.Dropout(rate=dropout_rate))     \n",
    "      \n",
    "#     # fourth series\n",
    "#     model.add(keras.layers.Flatten())         \n",
    "#     model.add(keras.layers.Dense(64, activation='sigmoid')) # 64\n",
    "#     # add a dropout layer if rate is not null    \n",
    "#     if dropout_rate != 0:\n",
    "#         model.add(keras.layers.Dropout(rate=dropout_rate)) \n",
    "        \n",
    "#     model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "#     # Compile model\n",
    "#     model.compile( \n",
    "#         optimizer='adam',\n",
    "#         loss='categorical_crossentropy',\n",
    "#         metrics=['accuracy'],\n",
    "#         )    \n",
    "#     return model\n",
    "\n",
    "# cnn = create_cnn_model()\n",
    "\n",
    "# cnn.compile(\n",
    "#   optimizer='adam',\n",
    "#   loss='categorical_crossentropy',  \n",
    "#   metrics=['accuracy'],\n",
    "# )\n",
    "\n",
    "# cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optimize model \n",
    "\n",
    "\n",
    "# n_epochs = 30 # 30 \n",
    "# n_epochs_cv = 10 # 10  # reduce number of epochs for cross validation for performance reason\n",
    "\n",
    "# n_cv = 3\n",
    "# validation_ratio = 0.10\n",
    "\n",
    "# # create model\n",
    "# model = KerasClassifier(build_fn=create_cnn_model, verbose=1)\n",
    "# # define parameters and values for grid search \n",
    "# param_grid = {\n",
    "#     'pool_type': ['max', 'average'],\n",
    "#     'conv_activation': ['sigmoid', 'tanh'],    \n",
    "#     'epochs': [n_epochs_cv],\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=n_cv)\n",
    "# grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optimize parameters of the fit method \n",
    "# cnn_model = create_cnn_model(pool_type = grid_result.best_params_['pool_type'],\n",
    "#                              conv_activation = grid_result.best_params_['conv_activation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Performance\n",
    "\n",
    "# history.history.keys()\n",
    "# pd.DataFrame(history.history).plot(figsize=(20, 16))\n",
    "# plt.grid(True)\n",
    "# plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "# plt.show()\n",
    "\n",
    "# # Performance\n",
    "# y_pred=cnn_model.predict(X_test) \n",
    "# y_pred=np.argmax(y_pred, axis=1)\n",
    "# y_test=np.argmax(y_test, axis=1)\n",
    "# confusion_matrix(y_test, y_pred)\n",
    "# # print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tailoring Gird Search to our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the model for Keras wrapper to scikit learn\n",
    "\n",
    "# We will optimize the type of pooling layer (max or average), the number of middle layers,\n",
    "# the activation function of the 2nd and 3rd convolution layers, the dropout rate, the kernel size, \n",
    "# the number of neurons, and the optimizer\n",
    "\n",
    "def create_cnn_model(pool_type='max', conv_activation='sigmoid', dropout_rate=0.1, neurons=16, kern_size=2,\n",
    "                    optimizer='adam', mid_layers=1):\n",
    "    # Create model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # First layer: convolution\n",
    "    model.add(keras.layers.Conv2D(neurons, kernel_size=(kern_size, kern_size), activation='relu', input_shape=input_shape)) \n",
    "        \n",
    "    # First middle layer: convolution, pooling, and dropout\n",
    "    if mid_layers==1:\n",
    "        model.add(keras.layers.Conv2D(2*neurons, kernel_size=(kern_size,kern_size), activation=conv_activation))  \n",
    "        if pool_type == 'max':\n",
    "            model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        if pool_type == 'average':\n",
    "            model.add(keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "        if dropout_rate != 0:\n",
    "            model.add(keras.layers.Dropout(rate=dropout_rate))     \n",
    "    \n",
    "    # Second middle layer: convolution, pooling, and dropout    \n",
    "    if mid_layers==2:\n",
    "        model.add(keras.layers.Conv2D(4*neurons, kernel_size=(kern_size, kern_size), activation=conv_activation))   \n",
    "        if pool_type == 'max':\n",
    "            model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        if pool_type == 'average':\n",
    "            model.add(keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "        if dropout_rate != 0:\n",
    "            model.add(keras.layers.Dropout(rate=dropout_rate))     \n",
    "      \n",
    "    # Penultimate layer with Flatten, Dense, and Dropout\n",
    "    model.add(keras.layers.Flatten())         \n",
    "    model.add(keras.layers.Dense(4*neurons, activation='sigmoid')) \n",
    "    # add a dropout layer if rate is not null    \n",
    "    if dropout_rate != 0:\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate)) \n",
    "    \n",
    "    # Final layer with the softmax for classification\n",
    "    model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile( \n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        )    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 19, 19, 16)        80        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 18, 18, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 9, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 9, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2592)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                165952    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 168,307\n",
      "Trainable params: 168,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating and compiling the model with a summary\n",
    "\n",
    "cnn = create_cnn_model()\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the model with grid search\n",
    "\n",
    "# Grid search parameters\n",
    "n_epochs_cv = 10 # Number of epochs for our grid search\n",
    "n_cv = 3 # Number of cross validations\n",
    "\n",
    "# Create model to feed to our parameter grid search\n",
    "model = KerasClassifier(build_fn=create_cnn_model, verbose=1)\n",
    "\n",
    "# Define parameters and values for grid search to check in our model\n",
    "param_grid = {\n",
    "    # The features to range over in our model\n",
    "    'pool_type': ['max', 'average'],\n",
    "    'conv_activation': ['sigmoid', 'tanh'],\n",
    "    'neurons': [16,32],\n",
    "    'dropout_rate':[0.1,0.2,0.3,0.4,0.5],\n",
    "    'optimizer':['adam', 'RMSprop', 'SGD'],\n",
    "    'kern_size':[2,3,4],\n",
    "    'mid_layers':[0,1,2],\n",
    "    # The number of epochs for each model\n",
    "    'epochs': [n_epochs_cv],\n",
    "}\n",
    "\n",
    "# Creating the grid\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=n_cv)\n",
    "\n",
    "# Fitting the grid\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and compiling a model with the best parameters\n",
    "\n",
    "cnn_model = create_cnn_model(pool_type = grid_result.best_params_['pool_type'],\n",
    "                             conv_activation = grid_result.best_params_['conv_activation'],\n",
    "                            neurons=grid_result.best_params_['neurons'],\n",
    "                            dropout_rate=grid_result.best_params_['dropout_rate'],\n",
    "                            optimizer=grid_result.best_params_['optimizer'],\n",
    "                            kern_size=grid_result.best_params_['kern_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "\n",
    "\n",
    "# Fit parameters\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "val_split=0.1\n",
    "\n",
    "# Fitting\n",
    "history=cnn_model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=val_split, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance\n",
    "\n",
    "history.history.keys()\n",
    "pd.DataFrame(history.history).plot(figsize=(20, 16))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()\n",
    "\n",
    "# Performance\n",
    "y_pred=cnn_model.predict(X_test) \n",
    "y_pred=np.argmax(y_pred, axis=1)\n",
    "y_test=np.argmax(y_test, axis=1)\n",
    "confusion_matrix(y_test, y_pred)\n",
    "# print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a grid search for model and optimizer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the model for Keras wrapper to scikit learn\n",
    "\n",
    "# Now, we will run grid search on the optimizer. I NEED TO FILL IN THE PARAMETERS FROM THE GRID SEARCH!\n",
    "\n",
    "def create_cnn_model(pool_type='max', conv_activation='sigmoid', dropout_rate=0.1, neurons=16, kern_size=2,\n",
    "                    mid_layers=1, optimizer='adam', init_lr=1e-1, decay_steps=5000, decay_rates=0.1, ):\n",
    "    # Create model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # First layer: convolution\n",
    "    model.add(keras.layers.Conv2D(neurons, kernel_size=(kern_size, kern_size), activation='relu', input_shape=input_shape)) \n",
    "        \n",
    "    # First middle layer: convolution, pooling, and dropout\n",
    "    if mid_layers==1:\n",
    "        model.add(keras.layers.Conv2D(2*neurons, kernel_size=(kern_size,kern_size), activation=conv_activation))  \n",
    "        if pool_type == 'max':\n",
    "            model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        if pool_type == 'average':\n",
    "            model.add(keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "        if dropout_rate != 0:\n",
    "            model.add(keras.layers.Dropout(rate=dropout_rate))     \n",
    "    \n",
    "    # Second middle layer: convolution, pooling, and dropout    \n",
    "    if mid_layers==2:\n",
    "        model.add(keras.layers.Conv2D(4*neurons, kernel_size=(kern_size, kern_size), activation=conv_activation))   \n",
    "        if pool_type == 'max':\n",
    "            model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        if pool_type == 'average':\n",
    "            model.add(keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "        if dropout_rate != 0:\n",
    "            model.add(keras.layers.Dropout(rate=dropout_rate))  \n",
    "        \n",
    "    # Third middle layer: convolution, pooling, and dropout    \n",
    "    if mid_layers==3:\n",
    "        model.add(keras.layers.Conv2D(4*neurons, kernel_size=(kern_size, kern_size), activation=conv_activation))   \n",
    "        if pool_type == 'max':\n",
    "            model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        if pool_type == 'average':\n",
    "            model.add(keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "        if dropout_rate != 0:\n",
    "            model.add(keras.layers.Dropout(rate=dropout_rate)) \n",
    "      \n",
    "    # Penultimate layer with Flatten, Dense, and Dropout\n",
    "    model.add(keras.layers.Flatten())         \n",
    "    model.add(keras.layers.Dense(4*neurons, activation='sigmoid')) \n",
    "    # add a dropout layer if rate is not null    \n",
    "    if dropout_rate != 0:\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate)) \n",
    "    \n",
    "    # Final layer with the softmax for classification\n",
    "    model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Running through the optimizers\n",
    "    if optimizer=='adam':\n",
    "        # Learning Rate Schedule\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rates)\n",
    "    \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "    if optimizer=='RMSprop':\n",
    "        # Learning Rate Schedule\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rates)\n",
    "    \n",
    "        optimizer = keras.optimizers.RMSProp(learning_rate=lr_schedule)\n",
    "        \n",
    "    if optimizer=='SGD':\n",
    "        # Learning Rate Schedule\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rates)\n",
    "    \n",
    "        optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "        \n",
    "    # Compile model\n",
    "    model.compile( \n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        )    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the model with grid search\n",
    "\n",
    "# Grid search parameters\n",
    "n_epochs_cv = 20 # Number of epochs for our grid search\n",
    "n_cv = 3 # Number of cross validations\n",
    "\n",
    "# Create model to feed to our parameter grid search\n",
    "model = KerasClassifier(build_fn=create_mlp_model_optimizer, verbose=1)\n",
    "\n",
    "# Define parameters and values for grid search to check in our model\n",
    "param_grid = {\n",
    "    # The features to range over in our model\n",
    "    'pool_type': ['max', 'average'],\n",
    "    'conv_activation': ['sigmoid', 'tanh'],\n",
    "    'neurons': [16,32],\n",
    "    'dropout_rate':[0.1,0.2,0.3,0.4,0.5],\n",
    "    'optimizer':['adam', 'RMSprop', 'SGD'],\n",
    "    'kern_size':[2,3,4],\n",
    "    'mid_layers':[0, 1, 2, 3],\n",
    "     # The features to range over for our optimizer\n",
    "    'init_lr':[1e-1,1e-2,1e-3,1e-4,1e-5],\n",
    "    'decay_steps':range(1000,10000,1000),\n",
    "    'decay_rates':[.5,.6,.7,.8,.9],\n",
    "    # The number of epochs for each model\n",
    "    'epochs': [n_epochs_cv],\n",
    "}\n",
    "\n",
    "# Creating the grid\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=n_cv)\n",
    "\n",
    "# Fitting the grid\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and compiling a model with the best parameters\n",
    "\n",
    "cnn_model = create_cnn_model(pool_type = grid_result.best_params_['pool_type'],\n",
    "                             conv_activation = grid_result.best_params_['conv_activation'],\n",
    "                            neurons=grid_result.best_params_['neurons'],\n",
    "                            dropout_rate=grid_result.best_params_['dropout_rate'],\n",
    "                            optimizer=grid_result.best_params_['optimizer'],\n",
    "                            kern_size=grid_result.best_params_['kern_size'],\n",
    "                             init_lr=grid_result.best_params_['init_lr'],\n",
    "                          decay_steps=grid_result.best_params_['decay_steps'],\n",
    "                          decay_rates=grid_result.best_params_['decay_rates'])\n",
    "# Fitting the model\n",
    "history = cnn_model.fit(X_train, y_train, epochs=50, validation_data=(X_val,y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance\n",
    "\n",
    "history.history.keys()\n",
    "pd.DataFrame(history.history).plot(figsize=(20, 16))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred,axis=1) # Note, I need this for our accuracy and confusion matrix.\n",
    "\n",
    "\n",
    "# Score\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Our model\n",
    "\n",
    "# model = keras.Sequential(\n",
    "# [\n",
    "# keras.Input(shape=input_shape),\n",
    "# keras.layers.Conv2D(256, kernel_size=(2, 2), padding='same', activation=\"relu\"),\n",
    "# keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "# keras.layers.Conv2D(256, kernel_size=(2, 2), padding='same', activation=\"relu\"),\n",
    "# keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "# keras.layers.Conv2D(256, kernel_size=(2, 2), padding='same', activation=\"relu\"),\n",
    "# keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "# keras.layers.Flatten(),\n",
    "# keras.layers.Dropout(0.4),\n",
    "# keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "# ]\n",
    "# )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compiling and fitting our model\n",
    "\n",
    "# batch_size = 128\n",
    "# epochs = 20\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "# history=model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9-16: Kernel of 3x3 seems to be doing well. Two layers, 256 neurons each, padding same, and maxpooling 2x2. It has begun to overfit, however. Upping the dropout to .2. Still overfitting after about epoch 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Performance\n",
    "\n",
    "# history.history.keys()\n",
    "# pd.DataFrame(history.history).plot(figsize=(20, 16))\n",
    "# plt.grid(True)\n",
    "# plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "# plt.show()\n",
    "\n",
    "# # Performance\n",
    "# y_pred=model.predict(X_test) \n",
    "# y_pred=np.argmax(y_pred, axis=1)\n",
    "# y_test=np.argmax(y_test, axis=1)\n",
    "# confusion_matrix(y_test, y_pred)\n",
    "# # print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
