{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to import\n",
    "\n",
    "# Standard data, plotting, and mathematical tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Statistical Tools\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "from keras import utils\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import tensorflow as tf\n",
    "\n",
    "# Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "\n",
    "dfs=['Non-Scaled TA Features 1H for BTC.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X, y, Train, Validation, and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to create PCA matrices\n",
    "\n",
    "def PCA_creation_train_val_test(no_components, X_train, X_val, X_test):\n",
    "    \n",
    "    # Scaling the data with our X_train matrix\n",
    "    scaler=StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Fitting the PCA to our X_train matrix\n",
    "    pca=PCA(n_components=no_components)\n",
    "    X_train=pca.fit_transform(X_train)\n",
    "\n",
    "    # Scaling the X_val and X_test \n",
    "    X_val=scaler.transform(X_val)\n",
    "    X_test=scaler.transform(X_test)\n",
    "\n",
    "    # Transforming the X_val and X_test\n",
    "    X_val=pca.transform(X_val)\n",
    "    X_test=pca.transform(X_test)\n",
    "    \n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splitting and scaling\n",
    "X=pd.read_csv(dfs[0])\n",
    "X=X.dropna()\n",
    "y=X['Label']\n",
    "X=X.drop('Label', axis=1)\n",
    "X=X.drop('Unnamed: 0', axis=1)\n",
    "X=X.drop('Percent Change', axis=1)\n",
    "\n",
    "# Split into train, val, and test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,test_size=0.1, random_state=100, shuffle=False)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_train, y_train, test_size=0.1, random_state=100, shuffle=False)\n",
    "\n",
    "\n",
    "X_train, X_val, X_test=PCA_creation_train_val_test(20, X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Model Identification: The partial auto-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated partial auto-correlation function (PACF) can be used to identify the order of an autoregressive time series model. Values of $|\\tau_h|$ greater or equal to $\\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{T}}$, where $T$ is the number of observations and $\\Phi(z)$ is the standard normal CDF, are significant lag $h$ partial autocorelations at the $\\alpha$ confidence level.\n",
    "\n",
    "We use the stattools package to estimate the PACF. The nlags parameter is the maximum number of lags used for PACF estimation.\n",
    "\n",
    "We will then use this to determine the number of look-back steps for the most important feature, our first PCA component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps set to 3\n"
     ]
    }
   ],
   "source": [
    "# Finding the best number of steps for our dataset\n",
    "\n",
    "pacf = sm.tsa.stattools.pacf(X_train[:,0], nlags=8)\n",
    "T = len(X_train[:,0])\n",
    "\n",
    "sig_test = lambda tau_h: np.abs(tau_h) > 2.58/np.sqrt(T)\n",
    "\n",
    "for i in range(len(pacf)):\n",
    "    if sig_test(pacf[i]) == False:\n",
    "        n_steps = i - 1\n",
    "        print('n_steps set to', n_steps)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test, and Validation Lagged Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30939, 3, 20)\n"
     ]
    }
   ],
   "source": [
    "## Getting our training, validation, and testing sets\n",
    "\n",
    "# Lagging function\n",
    "def lagged_matrices(n_steps, X_matrix, y_matrix):\n",
    "    X_lagged=[]\n",
    "    y_lagged=[]\n",
    "    y_matrix=y_matrix.values\n",
    "    for i in range(n_steps, X_matrix.shape[0]):\n",
    "        lag_set=X_matrix[i-n_steps:i]\n",
    "        X_lagged.append(lag_set) # We are taking the last n_input to the present time periods as the \n",
    "        # X values.\n",
    "        y_lagged.append(y_matrix[i])\n",
    "    X_lagged=np.array(X_lagged) # They need to be arrays\n",
    "    return X_lagged, y_lagged\n",
    "\n",
    "\n",
    "# Classes, sets, and features\n",
    "num_classes=2 \n",
    "n_steps = 3 # How many time periods into the past we will look. Our avg_steps was a float, so we convert\n",
    "# it to an integer\n",
    "n_features = len(X.iloc[0]) # Number of PCA features\n",
    "\n",
    "# Getting our lagged matrices\n",
    "X_train, y_train=lagged_matrices(n_steps, X_train, y_train)\n",
    "X_val, y_val=lagged_matrices(n_steps, X_val, y_val)\n",
    "X_test, y_test=lagged_matrices(n_steps, X_test, y_test)\n",
    "\n",
    "# One-hot Encoding our y vectors\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_val = utils.to_categorical(y_val, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(X_train.shape) # Confirming that our shape is (n_instances, n_steps, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(activ_function='relu', neurons=50, dropout_rate=0.1, num_layers=1, \n",
    "                     optimizer='adam',init_lr=1e-1, decay_steps=5000, decay_rates=0.1):\n",
    "    # create model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    if num_layers==1:\n",
    "        #Adding the First input hidden layer and the LSTM layer\n",
    "        # return_sequences = True, means the output of every time step to be shared with hidden next layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=False)) # For the final later, we don't need to return the sequences\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "        \n",
    "    if num_layers==2:\n",
    "        #Adding the First input hidden layer and the LSTM layer\n",
    "        # return_sequences = True, means the output of every time step to be shared with hidden next layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=True))\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        # Second layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=False)) # For the final later, we don't need to return the sequences\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "        \n",
    "    if num_layers==3:\n",
    "        #Adding the First input hidden layer and the LSTM layer\n",
    "        # return_sequences = True, means the output of every time step to be shared with hidden next layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=True))\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        # Second layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=True))\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        # Third layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=False)) # For the final later, we don't need to return the sequences\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        \n",
    "    if num_layers==4:\n",
    "        #Adding the First input hidden layer and the LSTM layer\n",
    "        # return_sequences = True, means the output of every time step to be shared with hidden next layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=True))\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        # Second layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=True))\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        # Third layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=True))\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        # Fourth layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=False)) # For the final later, we don't need to return the sequences\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        \n",
    "    # Final layer with softmax for classification\n",
    "    model.add(keras.layers.Dense(3, activation=\"softmax\"))\n",
    "        \n",
    "    # Running through the optimizers\n",
    "    if optimizer=='adam':\n",
    "        # Learning Rate Schedule\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rates)\n",
    "    \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "    if optimizer=='RMSprop':\n",
    "        # Learning Rate Schedule\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rates)\n",
    "    \n",
    "        optimizer = keras.optimizers.RMSProp(learning_rate=lr_schedule)\n",
    "        \n",
    "    if optimizer=='SGD':\n",
    "        # Learning Rate Schedule\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rates)\n",
    "    \n",
    "        optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "        \n",
    "    # Compile model\n",
    "    model.compile( \n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        )    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                30400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 30,553\n",
      "Trainable params: 30,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating and compiling the model with a summary\n",
    "\n",
    "lstm = create_lstm_model()\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-8f6d72bc9ba8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Fitting the grid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Optimize the model with grid search\n",
    "\n",
    "# Grid search parameters\n",
    "n_epochs_cv = 20 # Number of epochs for our grid search\n",
    "n_cv = 3 # Number of cross validations\n",
    "\n",
    "# Create model to feed to our parameter grid search\n",
    "model = KerasClassifier(build_fn=create_lstm_model, verbose=1)\n",
    "\n",
    "# Define parameters and values for grid search to check in our model\n",
    "param_grid = {\n",
    "    # Model parameters\n",
    "    'activ_function':['relu', 'tanh', 'sigmoid'],\n",
    "    'neurons':[50,100,150,200,250,300],\n",
    "    'dropout_rate':[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'num_layers':[1, 2, 3],\n",
    "    'optimizer':['adam', 'RMSProp', 'SGD'],\n",
    "     # Optimizer parameters\n",
    "    'init_lr':[1e-1,1e-2,1e-3,1e-4,1e-5],\n",
    "    'decay_steps':range(1000,10000,1000),\n",
    "    'decay_rates':[.5,.6,.7,.8,.9],\n",
    "    # The number of epochs for each model\n",
    "    'epochs': [n_epochs_cv],\n",
    "}\n",
    "\n",
    "# Creating the grid\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=n_cv)\n",
    "\n",
    "# Fitting the grid\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model=create_lstm_model(activ_function=grid_result.best_params_['activ_function'], \n",
    "                                            neurons=grid_result.best_params_['neurons'], \n",
    "                                            dropout_rate=grid_result.best_params_['dropout_rate'],\n",
    "                                            num_layers=grid_result.best_params_['num_layers'],\n",
    "                                            optimizer=grid_result.best_params_['optimizer'],\n",
    "                          init_lr=grid_result.best_params_['init_lr'],\n",
    "                          decay_steps=grid_result.best_params_['decay_steps'],\n",
    "                          decay_rates=grid_result.best_params_['decay_rates'])\n",
    "\n",
    "# Fitting the model\n",
    "history = lstm_model.fit(X_train, y_train, epochs=50, validation_data=(X_val,y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Reloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "\n",
    "cnn_model.save('Models/LSTM Class 1H BTC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the model\n",
    "\n",
    "CNN_model_BTC=model = keras.models.load_model('Models/LSTM Class 1H BTC.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series Prediction Percent Change Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X and y Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (38201, 101)\n"
     ]
    }
   ],
   "source": [
    "# Creating feature and dep variable matrices\n",
    "X=pd.read_csv(dfs[0])\n",
    "y=X['Percent Change']\n",
    "X=X.drop('Label', axis=1)\n",
    "X=X.drop('Unnamed: 0', axis=1)\n",
    "X=X.drop('Close', axis=1)\n",
    "\n",
    "print('X shape', X.shape) # Confirming the shape of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, our prediction is not a class, but a regression, so we should check if our target series, y, is stationary using and Augmented Dickey-Fuller Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Dickey-Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF Statistic for BTC Closing Values is -27.765983580387037\n",
      "p-value for BTC Closing Values is 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = adfuller(y)\n",
    "print('ADF Statistic for BTC Closing Values is '+str(result[0]))\n",
    "print('p-value for BTC Closing Values is ' +str(result[1]))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a p-value of 0, we fail to reject the null hypothesis and will treat the percent changes of BTC as stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to create PCA matrices\n",
    "\n",
    "def PCA_creation_train_val_test(no_components, X_train, X_val, X_test):\n",
    "    \n",
    "    # Scaling the data with our X_train matrix\n",
    "    scaler=StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Fitting the PCA to our X_train matrix\n",
    "    pca=PCA(n_components=no_components)\n",
    "    X_train=pca.fit_transform(X_train)\n",
    "\n",
    "    # Scaling the X_val and X_test \n",
    "    X_val=scaler.transform(X_val)\n",
    "    X_test=scaler.transform(X_test)\n",
    "\n",
    "    # Transforming the X_val and X_test\n",
    "    X_val=pca.transform(X_val)\n",
    "    X_test=pca.transform(X_test)\n",
    "    \n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, val, and test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,test_size=0.1, random_state=100, shuffle=False)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_train, y_train, test_size=0.1, random_state=100, shuffle=False)\n",
    "\n",
    "\n",
    "X_train, X_val, X_test=PCA_creation_train_val_test(20, X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps for Percent Change set to 2\n",
      "n_steps for first PCA Component set to 3\n"
     ]
    }
   ],
   "source": [
    "# PACF for our Percent Change and first PCA Component\n",
    "\n",
    "# PACF for our Percent Change\n",
    "pacf = sm.tsa.stattools.pacf(y, nlags=30)\n",
    "T = len(y)\n",
    "\n",
    "sig_test = lambda tau_h: np.abs(tau_h) > 2.58/np.sqrt(T)\n",
    "\n",
    "for i in range(len(pacf)):\n",
    "    if sig_test(pacf[i]) == False:\n",
    "        n_steps = i - 1\n",
    "        print('n_steps for Percent Change set to', n_steps)\n",
    "        break\n",
    "        \n",
    "# PACF for our first PCA Component\n",
    "pacf = sm.tsa.stattools.pacf(X_train[:,0], nlags=8)\n",
    "T = len(X.iloc[:,0])\n",
    "\n",
    "sig_test = lambda tau_h: np.abs(tau_h) > 2.58/np.sqrt(T)\n",
    "\n",
    "for i in range(len(pacf)):\n",
    "    if sig_test(pacf[i]) == False:\n",
    "        n_steps = i - 1\n",
    "        print('n_steps for first PCA Component set to', n_steps)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to capture as much information as possible for our model, we will set the number of lags to 3 here, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test, and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30939, 3, 20)\n"
     ]
    }
   ],
   "source": [
    "## Getting our training, validation, and testing sets\n",
    "\n",
    "# Lagging function\n",
    "def lagged_matrices(n_steps, X_matrix, y_matrix):\n",
    "    X_lagged=[]\n",
    "    y_lagged=[]\n",
    "    y_matrix=y_matrix.values\n",
    "    for i in range(n_steps, X_matrix.shape[0]):\n",
    "        lag_set=X_matrix[i-n_steps:i]\n",
    "        X_lagged.append(lag_set) # We are taking the last n_input to the present time periods as the \n",
    "        # X values.\n",
    "        y_lagged.append(y_matrix[i])\n",
    "    X_lagged=np.array(X_lagged) # They need to be arrays\n",
    "    return X_lagged, y_lagged\n",
    "\n",
    "\n",
    "# Classes, sets, and features\n",
    "num_classes=2 \n",
    "n_steps = 3 # How many time periods into the past we will look. Our avg_steps was a float, so we convert\n",
    "# it to an integer\n",
    "n_features = len(X.iloc[0]) # Number of PCA features\n",
    "\n",
    "# Getting our lagged matrices\n",
    "X_train, y_train=lagged_matrices(n_steps, X_train, y_train)\n",
    "X_val, y_val=lagged_matrices(n_steps, X_val, y_val)\n",
    "X_test, y_test=lagged_matrices(n_steps, X_test, y_test)\n",
    "\n",
    "print(X_train.shape) # Confirming that our shape is (n_instances, n_steps, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(activ_function='relu', neurons=50, dropout_rate=0.1, num_layers=1, \n",
    "                     optimizer='adam',init_lr=1e-1, decay_steps=5000, decay_rates=0.1):\n",
    "    # create model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    if num_layers==1:\n",
    "        #Adding the First input hidden layer and the LSTM layer\n",
    "        # return_sequences = True, means the output of every time step to be shared with hidden next layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=False)) # For the final later, we don't need to return the sequences\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "        \n",
    "    if num_layers==2:\n",
    "        #Adding the First input hidden layer and the LSTM layer\n",
    "        # return_sequences = True, means the output of every time step to be shared with hidden next layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=True))\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        # Second layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=False)) # For the final later, we don't need to return the sequences\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "        \n",
    "    if num_layers==3:\n",
    "        #Adding the First input hidden layer and the LSTM layer\n",
    "        # return_sequences = True, means the output of every time step to be shared with hidden next layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=True))\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        # Second layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=True))\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        # Third layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=False)) # For the final later, we don't need to return the sequences\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        \n",
    "    if num_layers==4:\n",
    "        #Adding the First input hidden layer and the LSTM layer\n",
    "        # return_sequences = True, means the output of every time step to be shared with hidden next layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=True))\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        # Second layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=True))\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        # Third layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=True))\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        # Fourth layer\n",
    "        model.add(keras.layers.LSTM(units = neurons, activation = activ_function, input_shape = (n_steps, n_features), \n",
    "               return_sequences=False)) # For the final later, we don't need to return the sequences\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "        \n",
    "    # Final layer with softmax for classification\n",
    "    model.add(keras.layers.Dense(1))\n",
    "        \n",
    "    # Running through the optimizers\n",
    "    if optimizer=='adam':\n",
    "        # Learning Rate Schedule\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rates)\n",
    "    \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "    if optimizer=='RMSprop':\n",
    "        # Learning Rate Schedule\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rates)\n",
    "    \n",
    "        optimizer = keras.optimizers.RMSProp(learning_rate=lr_schedule)\n",
    "        \n",
    "    if optimizer=='SGD':\n",
    "        # Learning Rate Schedule\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rates)\n",
    "    \n",
    "        optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "        \n",
    "    # Compile model\n",
    "    model.compile( \n",
    "        optimizer=optimizer,\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['accuracy'],\n",
    "        )    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-8f6d72bc9ba8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Fitting the grid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Optimize the model with grid search\n",
    "\n",
    "# Grid search parameters\n",
    "n_epochs_cv = 20 # Number of epochs for our grid search\n",
    "n_cv = 3 # Number of cross validations\n",
    "\n",
    "# Create model to feed to our parameter grid search\n",
    "model = KerasClassifier(build_fn=create_lstm_model, verbose=1)\n",
    "\n",
    "# Define parameters and values for grid search to check in our model\n",
    "param_grid = {\n",
    "    # Model parameters\n",
    "    'activ_function':['relu', 'tanh', 'sigmoid'],\n",
    "    'neurons':[50,100,150,200,250,300],\n",
    "    'dropout_rate':[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'num_layers':[1, 2, 3],\n",
    "    'optimizer':['adam', 'RMSProp', 'SGD'],\n",
    "     # Optimizer parameters\n",
    "    'init_lr':[1e-1,1e-2,1e-3,1e-4,1e-5],\n",
    "    'decay_steps':range(1000,10000,1000),\n",
    "    'decay_rates':[.5,.6,.7,.8,.9],\n",
    "    # The number of epochs for each model\n",
    "    'epochs': [n_epochs_cv],\n",
    "}\n",
    "\n",
    "# Creating the grid\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=n_cv)\n",
    "\n",
    "# Fitting the grid\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1528/1528 [==============================] - 12s 6ms/step - loss: 2.8657 - val_loss: 5.7062\n",
      "Epoch 2/10\n",
      "1528/1528 [==============================] - 7s 4ms/step - loss: 0.7309 - val_loss: 1.2770\n",
      "Epoch 3/10\n",
      "1528/1528 [==============================] - 7s 4ms/step - loss: 0.4029 - val_loss: 0.6136\n",
      "Epoch 4/10\n",
      "1528/1528 [==============================] - 7s 4ms/step - loss: 0.4262 - val_loss: 0.8127\n",
      "Epoch 5/10\n",
      "1528/1528 [==============================] - 8s 5ms/step - loss: 0.4560 - val_loss: 0.2667\n",
      "Epoch 6/10\n",
      "1528/1528 [==============================] - 10s 7ms/step - loss: 0.3483 - val_loss: 0.5621\n",
      "Epoch 7/10\n",
      "1528/1528 [==============================] - 10s 6ms/step - loss: 0.2656 - val_loss: 0.6173\n",
      "Epoch 8/10\n",
      "1528/1528 [==============================] - 10s 7ms/step - loss: 0.2844 - val_loss: 0.9946\n",
      "Epoch 9/10\n",
      "1528/1528 [==============================] - 11s 7ms/step - loss: 0.3549 - val_loss: 1.6012\n",
      "Epoch 10/10\n",
      "1528/1528 [==============================] - 9s 6ms/step - loss: 0.2450 - val_loss: 5.3484\n"
     ]
    }
   ],
   "source": [
    "lstm_model=create_lstm_model(activ_function=grid_result.best_params_['activ_function'], \n",
    "                                            neurons=grid_result.best_params_['neurons'], \n",
    "                                            dropout_rate=grid_result.best_params_['dropout_rate'],\n",
    "                                            num_layers=grid_result.best_params_['num_layers'],\n",
    "                                            optimizer=grid_result.best_params_['optimizer'],\n",
    "                          init_lr=grid_result.best_params_['init_lr'],\n",
    "                          decay_steps=grid_result.best_params_['decay_steps'],\n",
    "                          decay_rates=grid_result.best_params_['decay_rates'])\n",
    "\n",
    "# Fitting the model\n",
    "history = lstm_model.fit(X_train, y_train, epochs=50, validation_data=(X_val,y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Reloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "\n",
    "lstm_model.save('Models/LSTM Reg 1H BTC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the model\n",
    "\n",
    "lstm_model_BTC=model = keras.models.load_model('Models/LSTM Reg 1H BTC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
