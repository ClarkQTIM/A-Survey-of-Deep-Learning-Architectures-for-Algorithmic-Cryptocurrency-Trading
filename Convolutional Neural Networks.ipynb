{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to import\n",
    "\n",
    "# Standard data, plotting, and mathematical tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# CNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Stats\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "\n",
    "dfs=['Non-Scaled TA Features 1H for BTC.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determing the number of lags for our images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Model Identification: The partial auto-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated partial auto-correlation function (PACF) can be used to identify the order of an autoregressive time series model. Values of $|\\tau_h|$ greater or equal to $\\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{T}}$, where $T$ is the number of observations and $\\Phi(z)$ is the standard normal CDF, are significant lag $h$ partial autocorelations at the $\\alpha$ confidence level.\n",
    "\n",
    "We use the stattools package to estimate the PACF. The nlags parameter is the maximum number of lags used for PACF estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to create PCA matrices\n",
    "\n",
    "def PCA_creation_train_val_test(no_components, X_train, X_val, X_test):\n",
    "    \n",
    "    # Scaling the data with our X_train matrix\n",
    "    scaler=StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Fitting the PCA to our X_train matrix\n",
    "    pca=PCA(n_components=no_components)\n",
    "    X_train=pca.fit_transform(X_train)\n",
    "\n",
    "    # Scaling the X_val and X_test \n",
    "    X_val=scaler.transform(X_val)\n",
    "    X_test=scaler.transform(X_test)\n",
    "\n",
    "    # Transforming the X_val and X_test\n",
    "    X_val=pca.transform(X_val)\n",
    "    X_test=pca.transform(X_test)\n",
    "    \n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splitting and scaling\n",
    "X=pd.read_csv(dfs[0])\n",
    "X=X.dropna()\n",
    "y=X['Label']\n",
    "X=X.drop('Label', axis=1)\n",
    "X=X.drop('Unnamed: 0', axis=1)\n",
    "X=X.drop('Percent Change', axis=1)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,test_size=0.1, random_state=100, shuffle=False)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_train, y_train, test_size=0.1, random_state=100, shuffle=False)\n",
    "\n",
    "\n",
    "X_train, X_val, X_test=PCA_creation_train_val_test(20, X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps set to 3\n"
     ]
    }
   ],
   "source": [
    "# Finding the best number of steps for our dataset\n",
    "\n",
    "pacf = sm.tsa.stattools.pacf(X_train[:,0], nlags=8)\n",
    "T = len(X_train[:,0])\n",
    "\n",
    "sig_test = lambda tau_h: np.abs(tau_h) > 2.58/np.sqrt(T)\n",
    "\n",
    "for i in range(len(pacf)):\n",
    "    if sig_test(pacf[i]) == False:\n",
    "        n_steps = i - 1\n",
    "        print('n_steps set to', n_steps)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating \"images\" of our sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (30939, 3, 20, 1)\n",
      "30939 train samples\n",
      "3818 val samples\n",
      "3435 test samples\n"
     ]
    }
   ],
   "source": [
    "lags=3\n",
    "\n",
    "\n",
    "# Image creation function\n",
    "def image_creation(lags,X):\n",
    "    X_images=[]\n",
    "    \n",
    "    for i in range(0, len(X)-lags):\n",
    "        img=X[i,:]\n",
    "        img=np.vstack((img, X[i+1,:]))\n",
    "        img=np.vstack((img, X[i+2,:]))\n",
    "        X_images.append(img)\n",
    "    return X_images\n",
    "    \n",
    "# Getting the X matrices' images\n",
    "X_train=image_creation(lags,X_train)\n",
    "X_val=image_creation(lags,X_val)\n",
    "X_test=image_creation(lags,X_test)\n",
    "    \n",
    "# Making sure our y vectors has the same length, now  \n",
    "y_train=y_val[:-lags]\n",
    "y_val=y_val[:-lags]\n",
    "y_test=y_val[:-lags]\n",
    "\n",
    "# Finalizing the shapes\n",
    "num_classes = 2\n",
    "input_shape = (lags, len(X.iloc[0]), 1)\n",
    "\n",
    "# Add a dimension at the end to make sure images have shape (3, 20, 1)\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_val = np.expand_dims(X_val, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_val.shape[0], \"val samples\")\n",
    "print(X_test.shape[0], \"test samples\")\n",
    "# one-hot-encode the target variable\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_val = utils.to_categorical(y_val, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the model for Keras wrapper to scikit learn\n",
    "\n",
    "# Now, we will run grid search on the optimizer. I NEED TO FILL IN THE PARAMETERS FROM THE GRID SEARCH!\n",
    "\n",
    "def create_cnn_model(pool_type='max', conv_activation='sigmoid', dropout_rate=0.1, neurons=16, kern_size=2,\n",
    "                    mid_layers=1, optimizer='adam', init_lr=1e-1, decay_steps=5000, decay_rates=0.1, ):\n",
    "    # Create model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # First layer: convolution\n",
    "    model.add(keras.layers.Conv2D(neurons, kernel_size=(kern_size, kern_size), activation='relu', input_shape=input_shape)) \n",
    "        \n",
    "    # First middle layer: convolution, pooling, and dropout\n",
    "    if mid_layers==1:\n",
    "        model.add(keras.layers.Conv2D(2*neurons, kernel_size=(kern_size,kern_size), activation=conv_activation))  \n",
    "        if pool_type == 'max':\n",
    "            model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        if pool_type == 'average':\n",
    "            model.add(keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "        if dropout_rate != 0:\n",
    "            model.add(keras.layers.Dropout(rate=dropout_rate))     \n",
    "    \n",
    "    # Second middle layer: convolution, pooling, and dropout    \n",
    "    if mid_layers==2:\n",
    "        model.add(keras.layers.Conv2D(4*neurons, kernel_size=(kern_size, kern_size), activation=conv_activation))   \n",
    "        if pool_type == 'max':\n",
    "            model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        if pool_type == 'average':\n",
    "            model.add(keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "        if dropout_rate != 0:\n",
    "            model.add(keras.layers.Dropout(rate=dropout_rate))  \n",
    "        \n",
    "    # Third middle layer: convolution, pooling, and dropout    \n",
    "    if mid_layers==3:\n",
    "        model.add(keras.layers.Conv2D(4*neurons, kernel_size=(kern_size, kern_size), activation=conv_activation))   \n",
    "        if pool_type == 'max':\n",
    "            model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        if pool_type == 'average':\n",
    "            model.add(keras.layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "        if dropout_rate != 0:\n",
    "            model.add(keras.layers.Dropout(rate=dropout_rate)) \n",
    "      \n",
    "    # Penultimate layer with Flatten, Dense, and Dropout\n",
    "    model.add(keras.layers.Flatten())         \n",
    "    model.add(keras.layers.Dense(4*neurons, activation='sigmoid')) \n",
    "    # add a dropout layer if rate is not null    \n",
    "    if dropout_rate != 0:\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate)) \n",
    "    \n",
    "    # Final layer with the softmax for classification\n",
    "    model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Running through the optimizers\n",
    "    if optimizer=='adam':\n",
    "        # Learning Rate Schedule\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rates)\n",
    "    \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "    if optimizer=='RMSprop':\n",
    "        # Learning Rate Schedule\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rates)\n",
    "    \n",
    "        optimizer = keras.optimizers.RMSProp(learning_rate=lr_schedule)\n",
    "        \n",
    "    if optimizer=='SGD':\n",
    "        # Learning Rate Schedule\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rates)\n",
    "    \n",
    "        optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "        \n",
    "    # Compile model\n",
    "    model.compile( \n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        )    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the model with grid search\n",
    "\n",
    "# Grid search parameters\n",
    "n_epochs_cv = 20 # Number of epochs for our grid search\n",
    "n_cv = 3 # Number of cross validations\n",
    "\n",
    "# Create model to feed to our parameter grid search\n",
    "model = KerasClassifier(build_fn=create_mlp_model_optimizer, verbose=1)\n",
    "\n",
    "# Define parameters and values for grid search to check in our model\n",
    "param_grid = {\n",
    "    # The features to range over in our model\n",
    "    'pool_type': ['max', 'average'],\n",
    "    'conv_activation': ['sigmoid', 'tanh'],\n",
    "    'neurons': [16,32],\n",
    "    'dropout_rate':[0.1,0.2,0.3,0.4,0.5],\n",
    "    'optimizer':['adam', 'RMSprop', 'SGD'],\n",
    "    'kern_size':[2,3,4],\n",
    "    'mid_layers':[0, 1, 2, 3],\n",
    "     # The features to range over for our optimizer\n",
    "    'init_lr':[1e-1,1e-2,1e-3,1e-4,1e-5],\n",
    "    'decay_steps':range(1000,10000,1000),\n",
    "    'decay_rates':[.5,.6,.7,.8,.9],\n",
    "    # The number of epochs for each model\n",
    "    'epochs': [n_epochs_cv],\n",
    "}\n",
    "\n",
    "# Creating the grid\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=n_cv)\n",
    "\n",
    "# Fitting the grid\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and compiling a model with the best parameters\n",
    "\n",
    "cnn_model = create_cnn_model(pool_type = grid_result.best_params_['pool_type'],\n",
    "                             conv_activation = grid_result.best_params_['conv_activation'],\n",
    "                            neurons=grid_result.best_params_['neurons'],\n",
    "                            dropout_rate=grid_result.best_params_['dropout_rate'],\n",
    "                            optimizer=grid_result.best_params_['optimizer'],\n",
    "                            kern_size=grid_result.best_params_['kern_size'],\n",
    "                             init_lr=grid_result.best_params_['init_lr'],\n",
    "                          decay_steps=grid_result.best_params_['decay_steps'],\n",
    "                          decay_rates=grid_result.best_params_['decay_rates'])\n",
    "# Fitting the model\n",
    "history = cnn_model.fit(X_train, y_train, epochs=50, validation_data=(X_val,y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance\n",
    "\n",
    "history.history.keys()\n",
    "pd.DataFrame(history.history).plot(figsize=(20, 16))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred,axis=1) # Note, I need this for our accuracy and confusion matrix.\n",
    "\n",
    "\n",
    "# Score\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "\n",
    "cnn_model.save('Models/CNN 1H BTC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the model\n",
    "\n",
    "cnn_model_BTC=model = keras.models.load_model('Models/CNN 1H BTC.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
